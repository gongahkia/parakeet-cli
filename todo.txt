(A) Implement directory scanner: accept path, recursively discover all `.parquet` files, detect Hive-style partitioning (parse `col=val/` directory segments), return Vec<ParquetFilePath> with partition key-value pairs +reader
(A) Implement parallel metadata reader: given Vec of Parquet file paths, read footer metadata from each file concurrently using `rayon` thread pool, aggregate into DatasetProfile struct containing total row count, file count, total bytes, combined schema +reader
(A) Implement schema extraction from Parquet footer: parse SchemaDescriptor into structured ColumnSchema with name, physical_type (INT32/INT64/FLOAT/DOUBLE/BYTE_ARRAY/etc), logical_type (STRING/DATE/TIMESTAMP/DECIMAL/etc), repetition (REQUIRED/OPTIONAL/REPEATED), max_def_level, max_rep_level +schema
(A) Implement schema consistency checker across multiple files: compare schemas file-by-file, detect column additions/removals, type changes, nullability changes, return Vec<SchemaInconsistency> with file paths and diff descriptions +schema
(A) Implement metadata-only column statistics reader: extract min/max/null_count/distinct_count from ColumnChunk statistics in each RowGroup metadata — no data page reads required, return ColumnStats struct per column per row group +stats
(A) Implement aggregated column statistics: merge per-row-group ColumnStats into file-level stats — global min, global max, total null count, null percentage, total distinct count estimate, data page size total, compression ratio +stats
(A) Implement row group profiler from metadata: for each RowGroup extract num_rows, total_byte_size, total_compressed_size, compression ratio, column chunk offsets and sizes, return Vec<RowGroupProfile> +stats
(A) Implement row group size uniformity analysis: compute mean/median/stddev/min/max of row group sizes (both row count and byte size), flag outlier row groups (>2 stddev from mean) as potential compaction candidates +stats
(A) Implement encoding analysis per column: read Encodings from column chunk metadata (PLAIN, RLE, DICTIONARY, DELTA_BINARY_PACKED, etc), compute percentage of data using each encoding, flag columns using only PLAIN as potential optimization targets +stats
(A) Implement compression analysis per column: extract codec (SNAPPY/GZIP/ZSTD/LZ4/BROTLI/UNCOMPRESSED) from column chunks, compute per-column compression ratio (uncompressed_size/compressed_size), flag uncompressed columns +stats
(A) Implement full-scan mode column profiler: read actual data pages for selected columns using arrow RecordBatchReader with configurable batch size (default 65536 rows), compute statistics incrementally without loading entire column into memory +profile
(A) Implement streaming cardinality estimator using HyperLogLog (precision 14): feed column values through HLL as batches are read, return approximate distinct count with error bounds — works for any column type without materializing full value set +profile
(A) Implement value frequency counter for low-cardinality columns: if HLL estimate < 10000 distinct values, build exact frequency HashMap, return top-N most frequent values with counts and percentages (default N=20) +profile
(A) Implement numeric column distribution profiler: for INT/FLOAT/DOUBLE columns, compute mean, median (via t-digest approximate quantiles), stddev, p1/p5/p25/p50/p75/p95/p99 percentiles, skewness, kurtosis — all streaming single-pass +profile
(A) Implement histogram builder for numeric columns: compute equal-width histogram with configurable bin count (default 30), return Vec<HistogramBin> with range_start, range_end, count — used for TUI visualization +profile
(A) Implement string column profiler: compute min/max length, mean length, empty string count, whitespace-only count, pattern detection (all-numeric, email-like, UUID-like, ISO date-like) with percentage breakdown +profile
(A) Implement temporal column profiler: for DATE/TIMESTAMP columns, compute min/max date, date range span, null rate, distribution by year/month bucketed histogram +profile
(A) Implement boolean column profiler: compute true count, false count, null count, true percentage — simple but needs explicit handling +profile
(A) Implement data quality scoring per column: composite score 0-100 based on null rate (penalty above 5%), cardinality ratio (flag if = row count for non-ID columns), constant columns (cardinality=1), encoding efficiency — return QualityScore with breakdown +quality
(A) Implement dataset-level quality summary: aggregate column quality scores into overall dataset health score, flag top 5 worst columns, compute total null cell percentage, schema consistency score across files +quality
(A) Implement duplicate row detection using streaming hash: compute xxHash of concatenated column values per row, track in probabilistic set (bloom filter, 1% FP rate), report estimated duplicate percentage +quality
(A) Implement Ratatui TUI scaffold: async tokio runtime, crossterm event loop handling keyboard input + resize, tick-based UI refresh at 15Hz +tui
(A) Build TUI main layout with 4 zones: top bar (file path + dataset summary metrics + profiling mode badge), left sidebar (file tree / column list, scrollable), main content area (dynamic based on selected view), bottom bar (keybindings help + status) +tui
(A) Implement file overview view (default on launch): display file count, total rows, total size, size on disk, compression ratio, created_by, Parquet version, key-value metadata table — all from metadata-only read, instant load +tui
(A) Implement schema view: keybinding `S` shows full schema as indented tree (nested types expanded), each column shows name, physical type, logical type, repetition, with color coding by type family (numeric=cyan, string=green, temporal=yellow, boolean=magenta) +tui
(A) Implement column list in left sidebar: scrollable list of all columns with name, type icon, null% sparkline bar, quality score badge — j/k navigation, Enter to select column for detail view +tui
(A) Implement column detail view in main area: when column selected, show full statistics panel — type, encoding, compression, null rate, cardinality, min/max, mean/median/stddev (if numeric), top values (if categorical), quality score with breakdown +tui
(A) Implement histogram widget: render horizontal bar chart using unicode block elements (▏▎▍▌▋▊▉█) for numeric distribution, auto-scale to pane width, show bin range labels on y-axis, count on x-axis +tui
(A) Implement row group view: keybinding `R` shows table of all row groups with columns: index, num_rows, byte_size, compressed_size, compression_ratio — sortable by any column via `<`/`>` keys, outliers highlighted in red +tui
(A) Implement row group size distribution chart: horizontal bar chart showing byte size of each row group, mean line overlay, color gradient from green (near mean) to red (outlier) +tui
(A) Implement null rate heatmap view: keybinding `N` shows grid with columns on x-axis, row groups on y-axis, cell color intensity proportional to null percentage (white=0%, red=100%), useful for spotting systematic null patterns +tui
(A) Implement data preview view: keybinding `D` reads first N rows (default 100, configurable) from file, display as scrollable table with column headers, horizontal scroll for wide tables, cell values truncated to fit +tui
(A) Implement profiling mode toggle: keybinding `m` cycles between "metadata" (instant, footer only) and "full-scan" (reads data pages), show current mode in top bar badge, prompt confirmation before full-scan on files >1GB +tui
(A) Implement async profiling with progress: full-scan profiling runs on background tokio task, TUI shows progress bar (rows processed / total rows), estimated time remaining, cancel with Esc +tui
(A) Implement Tab key to cycle focus between sidebar, main content, and any active overlay +tui
(A) Add keybinding `q` to quit, `?` for help overlay listing all keybindings grouped by view +tui
(A) Wire startup flow: parse CLI path → scan files → read metadata (parallel) → launch TUI with file overview → sidebar populated with columns → ready for navigation +cli
(B) Implement S3 path resolver: parse `s3://bucket/prefix` URIs, use `aws-sdk-s3` crate with credential chain (env vars → profile → instance metadata), list objects matching prefix, return as virtual file list +reader
(B) Implement S3 range-read for Parquet footer: read only last 8 bytes (footer length) + footer bytes via HTTP Range request, parse metadata without downloading full file — critical for profiling remote files +reader
(B) Implement S3 column chunk selective read: for full-scan mode on remote files, read only selected column chunks via Range requests using offsets from metadata, avoid downloading entire file +reader
(B) Implement GCS path support: parse `gs://bucket/prefix` URIs, use `google-cloud-storage` crate or HTTP API with application default credentials +reader
(B) Implement comparison mode TUI: `compare` subcommand opens split-pane view, left = dataset A, right = dataset B, synchronized column selection — differences highlighted in yellow +compare
(B) Implement schema diff in comparison mode: side-by-side schema display, added columns in green, removed in red, type-changed in yellow, matching columns in default color +compare
(B) Implement statistics diff in comparison mode: for each common column, show delta between stats (null rate change, cardinality change, min/max drift, mean shift), flag significant changes (>10% relative change) in red +compare
(B) Implement row count and size comparison summary: show delta in total rows, file count, total size, compression ratio between two datasets — useful for detecting data loss or bloat between pipeline runs +compare
(B) Implement partition key analysis: for Hive-partitioned datasets, show partition column names, distinct partition values count, partition size distribution (rows and bytes per partition), flag skewed partitions (>3x median size) +stats
(B) Implement column correlation matrix for numeric columns: compute pairwise Pearson correlation using streaming algorithm, display as color-coded matrix in TUI (blue=negative, red=positive, white=zero) +profile
(B) Implement value length distribution histogram for string/binary columns: bucket by string length, render as histogram widget +profile
(B) Implement sorted order detection per column: sample first and last values from each row group, check if column appears sorted (ascending/descending), report as "appears sorted" badge — useful for predicate pushdown optimization hints +quality
(B) Implement page index analysis: if Parquet file has column index and offset index (Parquet 2.0 features), report coverage and effectiveness for predicate pushdown +stats
(B) Implement bloom filter presence detection: check if columns have bloom filter metadata, report which columns have bloom filters and estimated FP rate +stats
(B) Implement column size breakdown view: keybinding `Z` shows stacked bar chart of total byte size per column (compressed), sorted largest first — identifies columns dominating storage +tui
(B) Implement search within column list: keybinding `/` in sidebar activates text filter, fuzzy-match column names, filter list in real-time as user types +tui
(B) Implement column sorting in sidebar: keybinding `o` opens sort menu — sort by name (default), null rate, cardinality, size, quality score — ascending/descending toggle +tui
(B) Implement file list view for multi-file datasets: keybinding `F` shows scrollable file table with path, row count, size, row group count, schema hash — highlight files with schema inconsistencies +tui
(B) Implement encoding recommendation engine: for each column, suggest optimal encoding based on cardinality and data type (e.g., low cardinality string → DICTIONARY, sorted int → DELTA_BINARY_PACKED), show current vs recommended +quality
(B) Implement compression recommendation: compare current codec's ratio against estimated ratios for alternatives (ZSTD typically best general-purpose), flag columns where switching codec could save >20% space +quality
(B) Add `summary` subcommand headless output: print dataset profile as formatted table to stdout (file count, row count, columns, size, quality score, top issues), parseable with column alignment +cli
(B) Implement JSON export: `export --format json` writes full profile (schema, per-column stats, quality scores, row group profiles, issues) to structured JSON file +export
(B) Implement CSV export: `export --format csv` writes per-column statistics as CSV rows (column_name, type, null_rate, cardinality, min, max, mean, quality_score) +export
(B) Add `--columns <col1,col2,...>` flag to full-scan mode: only profile specified columns instead of all, reduces scan time for wide tables (100+ columns) +cli
(B) Implement color theme support: `[display].theme` in config with "dark" (default), "light", "nord", "catppuccin" palettes applied to all TUI widgets +tui
(B) Add bookmark columns: keybinding `b` on selected column adds to bookmarks, `B` shows bookmarked columns only in sidebar — useful for focusing on columns of interest in wide schemas +tui
(B) Implement TUI session state persistence: save last viewed file, selected column, active view, profiling mode to `~/.cache/parquet-lens/session.json`, restore on relaunch with same file +tui
(C) Implement predicate filter mode: keybinding `P` opens filter builder, accept SQL-like WHERE expression (e.g., `age > 30 AND status = 'active'`), re-profile only matching rows using arrow compute kernels +filter
(C) Implement predicate pushdown using Parquet row group statistics: skip row groups where min/max stats prove no rows can match predicate, report skipped vs scanned row groups +filter
(C) Implement filter expression parser: parse simple predicates (column op value) with AND/OR/NOT, operators: =, !=, <, >, <=, >=, IS NULL, IS NOT NULL, IN (...), LIKE — using pest or nom parser combinator +filter
(C) Implement sampling mode for full-scan: `--sample <percentage>` flag reads random N% of row groups instead of all, extrapolate statistics with confidence intervals — fast approximate profiling for very large files +profile
(C) Implement Parquet file repair suggestions: detect common issues (too many small row groups → suggest compaction, dictionary pages >1MB → suggest disabling dictionary, excessive null columns → suggest dropping), output as actionable recommendations list +quality
(C) Implement time-series aware profiling: detect timestamp columns, compute time gaps, identify missing intervals, check monotonicity, report temporal coverage — useful for event data +profile
(C) Implement nested type profiling: for columns with nested schemas (LIST, MAP, STRUCT), expand and profile child fields, show nesting depth and array length distribution +profile
(C) Implement diff against baseline: save a profile as baseline JSON, on next run compare current profile against baseline, flag regressions (quality score drops, new nulls, schema changes) +compare
(C) Add Parquet file creation metadata display: parse `created_by` field to identify writing engine (Spark, PyArrow, DuckDB, Impala), show engine-specific optimization hints +stats
(C) Implement cross-column null pattern analysis: detect columns that are always null together (null correlation), suggest they may be from same optional source/join — display as grouped null pattern in quality view +quality
