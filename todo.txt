


(B) Implement --watch for S3/GCS: when --watch is set and path is an S3/GCS URI spawn a background tokio task that re-fetches metadata every --watch-interval seconds (default 30) and sends reload events to the same channel as local watch +feature +core
(B) Add --watch-interval CLI arg to Inspect subcommand: u64 seconds default 5 for local and 30 for cloud; pass value into the watch loop implementation +feature +cli
(B) Fix DataPreview: in run_tui after opening file metadata read up to config.display.max_rows_preview rows using ParquetRecordBatchReaderBuilder with batch_size=max_rows_preview; populate app.preview_headers from schema field names and app.preview_rows from the first batch string-formatted values +bug +tui
(B) Fix run_compare S3/GCS path existence check: replace std::path::Path::new(&path1).exists() and path2 equivalent with an is_s3_uri/is_gcs_uri guard that skips the filesystem existence check for cloud URIs +bug +cli
(B) Create unified open_parquet_auto async fn in parquet-lens-core/src/reader.rs: dispatches to read_s3_parquet_metadata, read_gcs_parquet_metadata, or open_parquet_file based on URI prefix; returns (ParquetFileInfo, ParquetMetaData) with consistent ParquetFileInfo population for cloud paths +refactor +core
(B) Update run_summary to use open_parquet_auto via block_in_place instead of calling open_parquet_file directly enabling S3/GCS Summary paths +feature +cli
(B) Update run_compare to use open_parquet_auto for both path1 and path2 via block_in_place enabling cloud-path comparison +feature +cli
(B) Update run_export to use open_parquet_auto via block_in_place for cloud-path export support +feature +cli
(B) Update run_duplicates: for S3/GCS paths download file bytes via read_s3_range or GCS range request into a tempfile::NamedTempFile then pass the temp path to detect_duplicates instead of the URI string +feature +core
(B) Add check subcommand to Commands enum in main.rs: Commands::Check { path: String, format: String, fail_on_regression: bool } +feature +cli
(B) Implement run_check fn: calls rp(), load_file_stats, aggregate_column_stats, compute_quality_scores, load_baseline_regressions; prints regression list to stderr; if fail_on_regression and regressions non-empty returns Err to produce exit code 1 +feature +cli
(B) Add --format json flag to check subcommand: when format=json emit BaselineRegression list as JSON array to stdout instead of plain text for CI parsing +feature +cli
(B) Add --fail-on-regression flag to Inspect subcommand: after loading baseline in run_tui init block if regressions found and flag is set exit non-zero before launching TUI +feature +cli
(B) Fix sidebar responsive width: replace Constraint::Length(30) in render() layout with Constraint::Percentage(25) bounded by a minimum of 18 chars for the sidebar pane +bug +tui
(B) Add sidebar column name truncation: in render_sidebar compute available sidebar text width from the actual rendered Rect width and truncate column names to width-6 chars with a trailing ellipsis character when name exceeds available space +feature +tui
(B) Add [ and ] keybinds to resize sidebar width at runtime: store sidebar_width: u16 in App (default 30) clamped to 15..=60; persist to Session; apply in render() layout split +feature +tui
(B) Add config.display.sidebar_width: Option<u16> to DisplayConfig in parquet-lens-common/src/config.rs with default None (falls back to 30); read it in App::new to initialize sidebar_width +feature +common
(B) Add responsive narrow terminal mode: when terminal width < 80 columns hide sidebar entirely and show only the main panel; add backtick keybind to toggle sidebar visibility +feature +tui
(B) Fix V keybind blocking TUI: move detect_duplicates call in handle_sidebar V branch into spawn_blocking with a oneshot channel; add pending_duplicate_scan: bool and duplicate_rx: Option<Receiver<Result<DuplicateReport, String>>> to App; poll the receiver each tick like full-scan +bug +tui
(B) Fix schema consistency check O(n^2): in read_metadata_parallel replace ref_col_names Vec<&str> with HashSet<&str> and other_names Vec<&str> with HashSet<&str> for O(1) lookups in the schema diff loop +perf +core
(B) Fix filter_count double file open: open the file once call ParquetRecordBatchReaderBuilder::try_new() to get both metadata and the builder use .metadata() clone for schema/rg checks then .build() the reader from the same builder +perf +core
(B) Fix detect_repair_suggestions panic: add guard at top of fn returning empty Vec when rg_count == 0 before the integer division avg_bytes calculation and avg_page calculation +bug +core
(B) Fix summarize_quality schema_consistent: in run_summary replace the hardcoded true argument to summarize_quality with dataset.schema_inconsistencies.is_empty() +bug +cli
(B) Fix quality score metric: change summarize_quality overall_score from arithmetic mean to weighted mean by column total_data_page_size so high-null large columns penalize the score proportionally; accept agg_stats slice alongside quality scores +feature +core
(B) Fix worst_columns filter in summarize_quality: add .filter(|s| s.score < 80) before .take(5) so only genuinely poor columns appear in worst_columns list +bug +core
(B) Add bloom filter disclosure in Duplicates TUI view: in the render_duplicates panel add a static Line below estimated_duplicate_pct reading "(bloom filter ~1% false-positive rate -- use --exact flag for authoritative counts)" +feature +tui
(B) Add exact dedup path in detect_duplicates: for files where total_rows_estimate <= 5_000_000 use a HashSet<u64> instead of Bloom<u64> to produce exact counts; document the threshold in a comment and expose via exact: bool param +feature +core
(B) Add --exact flag to Duplicates subcommand: when set force HashSet-based exact dedup regardless of row count; otherwise use bloom filter heuristic +feature +cli
(B) Implement ProgressState::Cancelled: add Esc keybind handling during full-scan (when progress is Running) to set progress to Cancelled drop progress_rx and set pending_full_scan to false +feature +tui
(B) Implement compare_sidebar_col navigation in Compare view: when view == View::Compare and focus == Sidebar handle j/k/Up/Down to increment/decrement compare_sidebar_col; remove #[allow(dead_code)] attribute +feature +tui
(B) Add filter sample rows to FilterResult: add sample_rows: Vec<Vec<String>> (max 10) and sample_headers: Vec<String> fields to FilterResult struct; populate from first matching batch inside filter_count +feature +core
(B) Render filter sample rows in TUI: after a successful P-keybind filter display sample_rows in a scrollable table overlay or main panel instead of only updating status_msg +feature +tui
(B) Add --sample and --sample-seed flags to Summary subcommand: mirror Inspect flag definitions; pass SampleConfig to a new run_summary_sampled path that calls sample_row_groups before quality scoring +feature +cli
(B) Add --sample and --sample-seed flags to Export subcommand: when set run sample_row_groups first and export sampled agg_stats instead of full metadata stats +feature +cli
(B) Fix baseline key for S3/GCS: in BaselineProfile::save when path starts with s3:// or gs:// emit eprintln! warning noting that in-place file updates silently collide with this baseline key; add TODO comment recommending content-hash keying as a future improvement +stability +core
(B) Add PARQUET_LENS_CONFIG env var support: in Config::load check std::env::var("PARQUET_LENS_CONFIG") first and use that path if set before falling back to dirs::config_dir() path +feature +common
(B) Add SIGTERM handler: in run_tui register a ctrlc or signal-hook SIGTERM handler that calls disable_raw_mode() and execute!(LeaveAlternateScreen) before process::exit(0) to prevent terminal corruption on kill +stability +tui
(B) Fix profile_timeseries return type: change signature from anyhow::Result to Result<Vec<TimeSeriesProfile>, ParquetLensError> and update call site in run_tui +refactor +core
(B) Fix profile_timeseries INT96 detection: in run_tui ts_cols detection also match logical_type containing "Time" and add a fallback including columns with physical_type == "INT96" and None logical type as legacy Spark timestamp candidates +bug +core
(B) Fix timeseries error propagation: in run_tui Err branch of profile_timeseries match add eprintln!("timeseries warning: {e}") alongside setting status_msg so CLI/DevOps stderr captures it +bug +cli
(B) Fix nested_profiles error silently dropped: change if let Ok(np) = profile_nested_columns in run_tui to a match with an Err arm that calls eprintln! +bug +cli
(B) Add GcsConfig to Config: add pub gcs: GcsConfig with fields project_id: Option<String> and credentials_file: Option<String> to Config struct in parquet-lens-common/src/config.rs; update Config::default() +feature +common
(B) Thread GcsConfig into read_gcs_parquet_metadata: pass credentials_file path to reqwest client builder in gcs_reader.rs; if credentials_file is None attempt application-default credentials via GOOGLE_APPLICATION_CREDENTIALS env var +feature +core
(B) Fix parse_predicate escaped quotes: in tokenize() when inside a quoted string literal handle \' and \" escape sequences by consuming the backslash and pushing the following quote char without ending the token +bug +core
(B) Add cargo test step to ci.yml: add step running cargo test --workspace after the clippy step so tests are enforced on every push +infra
(B) Add cargo audit step to ci.yml: add step installing cargo-audit and running cargo audit to check for known CVEs in dependencies +infra
(B) Add CI matrix for macOS: extend ci.yml jobs.check.runs-on to a matrix of [ubuntu-latest, macos-latest] since the project targets macOS-primary users +infra
(B) Add unit tests for parse_predicate: in filter.rs #[cfg(test)] block test =, !=, <, <=, >, >= operators; IS NULL; IS NOT NULL; IN list; LIKE pattern; AND/OR/NOT combinations; and malformed inputs returning Err +test +core
(B) Add unit tests for score_column: in quality.rs #[cfg(test)] block test null_penalty calculation at 0% 5% 50% 100% null rates; constant column detection at distinct_count=0 and distinct_count=1; cardinality_flag when distinct==total_rows +test +core
(B) Add unit tests for like_match_at: in filter.rs #[cfg(test)] block test % wildcard at start/middle/end; _ single-char match; combined %_% pattern; empty string input; no-wildcard exact match +test +core
(B) Add unit tests for can_skip_row_group: in filter.rs #[cfg(test)] block test AND skip logic (either side false skips); OR skip logic (both sides false skips); NOT always returns false; Comparison skip for Int32/Int64 out-of-range values +test +core
(B) Add unit tests for identify_engine: in engine.rs #[cfg(test)] block test spark/pyarrow/duckdb/impala/hive/trino/flink/pandas/parquet-go/parquet4s created_by strings and unknown fallback returns empty hints +test +core
(B) Add integration test fixture: create tests/fixtures/ directory in parquet-lens-core with a minimal valid .parquet file; add integration test calling open_parquet_file, read_column_stats, aggregate_column_stats, score_column on the fixture +test +infra
(B) Add unit tests for detect_repair_suggestions: test zero row_groups returns empty Vec; test fragmentation trigger (>100 rgs avg <64MB); test high-null column suggestion (>50%); test large dict page suggestion +test +core
(B) Add parquet-lens filter subcommand: Commands::Filter { path: String, expr: String, output: Option<String>, limit: Option<usize> } that calls parse_predicate then filter_count and optionally exports matching rows to CSV +feature +cli
(B) Implement filter_rows fn in filter.rs: like filter_count but yields matching RecordBatches up to limit rows for use in filter subcommand CSV export path +feature +core
(B) Add parquet-lens schema subcommand: Commands::Schema { path: String, json: bool } that prints combined_schema as a plain table or JSON without full profiling for fast schema inspection in CI +feature +cli
(B) Fix full_scan_results not rendered in TUI: in render_column_detail when app.profiling_mode == FullScan and app.full_scan_results is non-empty display histogram bins and numeric profile data from the matching ColumnProfileResult for the selected column +bug +tui
(B) Add baseline_captured_at human-readable display: in render_baseline TUI view format app.baseline_captured_at unix timestamp using chrono::DateTime::from_timestamp as YYYY-MM-DD HH:MM:SS UTC string instead of raw seconds +feature +tui
(B) Add --json flag to Duplicates subcommand: emit DuplicateReport as serde_json to stdout when --json is passed +feature +cli
(B) Add --threshold flag to Duplicates subcommand: f64 percentage; exit non-zero if estimated_duplicate_pct exceeds threshold; enables use as pipeline quality gate +feature +cli
(B) Add --columns flag to Summary subcommand: Vec<String> via value_delimiter=','; filter column-level quality scores to named columns only in run_summary output +feature +cli
(B) Add --validate flag to Inspect subcommand: when set skip TUI launch; run detect_repair_suggestions, load_baseline_regressions, summarize_quality; print results to stdout; exit 0 if no issues exit 1 if issues found +feature +cli
(B) Add profile_timeseries output to export_json: include timeseries_profiles field in the JSON export document when timeseries data is non-empty +feature +core
(B) Add profile_nested_columns output to export_json: include nested_profiles field in JSON export document +feature +core
(B) Add repair_suggestions to export_json: include repair_suggestions field in JSON export document +feature +core
(B) Add row groups export to export_csv: write a second CSV file row_groups.csv to the same output directory containing RowGroupProfile data (index, row_count, total_byte_size, compression) +feature +core
(B) Fix filter_count column path comparison: replace schema_names string equality check with cm.column_descr().path_in_schema().string() comparison so nested column dot-notation paths are matched correctly +bug +core
(B) Add Decimal128 type support to build_mask in filter.rs: add downcast_ref::<Decimal128Array> branch with scale-aware f64 conversion for Eq/Ne/Lt/Le/Gt/Ge comparison ops +feature +core
(B) Add Date32/Date64 type support to build_mask in filter.rs: add downcast_ref::<Date32Array> and Date64Array branches comparing days-since-epoch as i64 values +feature +core
(B) Add BETWEEN predicate to parser: parse col BETWEEN a AND b emitting Predicate::And(Comparison(>=a), Comparison(<=b)) with full row-group pushdown via the two existing Comparison branches +feature +core
(B) Add NOT IN predicate to parser: parse col NOT IN (...) emitting Predicate::Not(In{...}); update can_skip_row_group to handle Not(In) conservatively returning false +feature +core
(B) Add SampleConfig seed documentation: add inline comment above SampleConfig struct and sample_row_groups fn stating seed: None produces non-deterministic results and CI pipelines must always pass --sample-seed for reproducibility +stability +core
(B) Move large-file threshold to config: replace hardcoded 1073741824 byte check in App::cycle_profiling_mode with config.profiling.large_file_threshold_bytes: u64 field (default 1073741824) defined in ProfilingConfig +refactor +common
(B) Add config.profiling.full_scan_timeout_secs: Option<u64> to ProfilingConfig; in run_tui spawn_blocking block use profile_columns_with_timeout instead of profile_columns when config value is Some +feature +common
(B) Add colorblind-safe theme: add colorblind variant to Theme::from_name that replaces red danger color with orange (#FF8C00) and green success with blue (#0080FF) +feature +tui
(B) Add --no-color flag to Summary subcommand: skip ANSI escape codes in pretty-format output when set; also honor NO_COLOR env var automatically via std::env::var("NO_COLOR").is_ok() at top of run_summary +feature +cli
(B) Add View::WatchLog to TUI: when --watch is active add W keybind rendering a scrollable list of last 20 reload events with timestamps and one-line diff summary (rows changed, schema changed flag) +feature +tui
(B) Add parquet-lens completions subcommand: Commands::Completions { shell: clap_complete::Shell } calling clap_complete::generate to stdout; add clap_complete to parquet-lens-tui Cargo.toml +feature +cli
(B) Fix session restore for FilterInput: verify App::to_session maps View::FilterInput to a serializable string and App::restore_from_session restores it; current code maps it to "overview" silently +bug +tui
(B) Add Makefile with standard targets: build (cargo build --release), test (cargo test --workspace), lint (cargo fmt --check && cargo clippy -- -D warnings), release (cargo build --release), install (cp target/release/parquet-lens /usr/local/bin/) +infra
(B) Add exit code documentation to check subcommand: add .long_about() on Commands::Check describing exit codes 0=clean 1=regressions-found 2=file-not-found for CI integration scripts +feature +cli
(B) Add --limit flag to Export subcommand: usize rows max; when set wrap the Arrow reader with .with_limit(limit) before exporting to CSV/JSON +feature +cli
(B) Add --format ndjson to Export subcommand: emit one JSON object per column stat as newline-delimited JSON for streaming CI tooling; add ndjson branch in run_export format match +feature +cli
(B) Fix sidebar_down/sidebar_up to operate on filtered index space: saturating_sub check max against filtered_column_indices().len() not raw column_count so navigation matches visible list +bug +tui
(B) Fix load_file_stats multi-file documentation: add doc comment on load_file_stats noting that returned meta is first-file-only and callers needing aggregate stats must use read_metadata_parallel separately +stability +core
(B) Add bloom filter size warning in detect_duplicates: if total_rows_estimate > 10_000_000 emit eprintln! warning before Bloom::new_for_fp_rate noting memory usage and recommending --exact flag +stability +core
(B) Wire profile_columns_with_timeout in run_tui spawn_blocking: when config.profiling.full_scan_timeout_secs is Some(t) call profile_columns_with_timeout with Duration::from_secs(t) instead of profile_columns +feature +tui
