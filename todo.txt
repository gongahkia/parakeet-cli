(B) Implement column size breakdown view: keybinding `Z` shows stacked bar chart of total byte size per column (compressed), sorted largest first — identifies columns dominating storage +tui
(B) Implement search within column list: keybinding `/` in sidebar activates text filter, fuzzy-match column names, filter list in real-time as user types +tui
(B) Implement column sorting in sidebar: keybinding `o` opens sort menu — sort by name (default), null rate, cardinality, size, quality score — ascending/descending toggle +tui
(B) Implement file list view for multi-file datasets: keybinding `F` shows scrollable file table with path, row count, size, row group count, schema hash — highlight files with schema inconsistencies +tui
(B) Implement encoding recommendation engine: for each column, suggest optimal encoding based on cardinality and data type (e.g., low cardinality string → DICTIONARY, sorted int → DELTA_BINARY_PACKED), show current vs recommended +quality
(B) Implement compression recommendation: compare current codec's ratio against estimated ratios for alternatives (ZSTD typically best general-purpose), flag columns where switching codec could save >20% space +quality
(B) Add `summary` subcommand headless output: print dataset profile as formatted table to stdout (file count, row count, columns, size, quality score, top issues), parseable with column alignment +cli
(B) Implement JSON export: `export --format json` writes full profile (schema, per-column stats, quality scores, row group profiles, issues) to structured JSON file +export
(B) Implement CSV export: `export --format csv` writes per-column statistics as CSV rows (column_name, type, null_rate, cardinality, min, max, mean, quality_score) +export
(B) Add `--columns <col1,col2,...>` flag to full-scan mode: only profile specified columns instead of all, reduces scan time for wide tables (100+ columns) +cli
(B) Implement color theme support: `[display].theme` in config with "dark" (default), "light", "nord", "catppuccin" palettes applied to all TUI widgets +tui
(B) Add bookmark columns: keybinding `b` on selected column adds to bookmarks, `B` shows bookmarked columns only in sidebar — useful for focusing on columns of interest in wide schemas +tui
(B) Implement TUI session state persistence: save last viewed file, selected column, active view, profiling mode to `~/.cache/parquet-lens/session.json`, restore on relaunch with same file +tui
(C) Implement predicate filter mode: keybinding `P` opens filter builder, accept SQL-like WHERE expression (e.g., `age > 30 AND status = 'active'`), re-profile only matching rows using arrow compute kernels +filter
(C) Implement predicate pushdown using Parquet row group statistics: skip row groups where min/max stats prove no rows can match predicate, report skipped vs scanned row groups +filter
(C) Implement filter expression parser: parse simple predicates (column op value) with AND/OR/NOT, operators: =, !=, <, >, <=, >=, IS NULL, IS NOT NULL, IN (...), LIKE — using pest or nom parser combinator +filter
(C) Implement sampling mode for full-scan: `--sample <percentage>` flag reads random N% of row groups instead of all, extrapolate statistics with confidence intervals — fast approximate profiling for very large files +profile
(C) Implement Parquet file repair suggestions: detect common issues (too many small row groups → suggest compaction, dictionary pages >1MB → suggest disabling dictionary, excessive null columns → suggest dropping), output as actionable recommendations list +quality
(C) Implement time-series aware profiling: detect timestamp columns, compute time gaps, identify missing intervals, check monotonicity, report temporal coverage — useful for event data +profile
(C) Implement nested type profiling: for columns with nested schemas (LIST, MAP, STRUCT), expand and profile child fields, show nesting depth and array length distribution +profile
(C) Implement diff against baseline: save a profile as baseline JSON, on next run compare current profile against baseline, flag regressions (quality score drops, new nulls, schema changes) +compare
(C) Add Parquet file creation metadata display: parse `created_by` field to identify writing engine (Spark, PyArrow, DuckDB, Impala), show engine-specific optimization hints +stats
(C) Implement cross-column null pattern analysis: detect columns that are always null together (null correlation), suggest they may be from same optional source/join — display as grouped null pattern in quality view +quality
